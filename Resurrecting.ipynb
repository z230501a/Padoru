{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resurrecting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/z230501a/Padoru/blob/master/Resurrecting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvrhnbbn8x2l",
        "colab_type": "text"
      },
      "source": [
        "利用機器學習模仿魔戒(朱學恆譯版)的文筆，並且給予開頭引導其能夠依照原文的思維接續下去。\n",
        "\n",
        "程式參考自[穿越時空的偉人：用PyTorch重現偉人們的神經網絡](https://medium.com/pyladies-taiwan/%E7%A9%BF%E8%B6%8A%E6%99%82%E7%A9%BA%E7%9A%84%E5%81%89%E4%BA%BA-%E7%94%A8pytorch%E9%87%8D%E7%8F%BE%E5%81%89%E4%BA%BA%E5%80%91%E7%9A%84%E7%A5%9E%E7%B6%93%E7%B6%B2%E7%B5%A1-bd045ac43e96?fbclid=IwAR2I3JlAGpjCxhDckpyI8nCFdeVr2uBm_orDVfOsQt2xncXlXPrCjUFuCwY)這篇文章，並且進行修改後將其使用於Colabotary上。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRFFEYITAiQQ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynR3bX1X80c1",
        "colab_type": "text"
      },
      "source": [
        "而為了進行，所以要先導入套件。所用的套件有以下，這次主要用到的是Pytorch。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY_ZO1jpIBlD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from io import BytesIO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRxWZ-Gf-8Lx",
        "colab_type": "text"
      },
      "source": [
        "**Pytorch介紹**\n",
        "\n",
        "Pytorch是一種機器學習框架，由Facebook開源，建立在Torch之上，然而已經與Torch相差甚大。\n",
        "\n",
        "標榜Python First，使用上與Python無異，並且能夠與其他Python套件整合。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUrTZi-T88nf",
        "colab_type": "text"
      },
      "source": [
        "學習的模式是，依照前面出現的句子，然後去找適合接續的字接續。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peEhhQ5dhxrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def parse_corpus(path, seq_length=50):\n",
        "    '''Parse raw corpus text into input-output pairs, where input is a sequence of characters, output is 1 character after the sequence'''\n",
        "\n",
        "    # Read text\n",
        "    with open(path, 'r') as f:\n",
        "        raw_text = f.read().replace('\\n', '')\n",
        "\n",
        "    # Get unique characters\n",
        "    chars = sorted(list(set(raw_text)))\n",
        "\n",
        "    # Map char to int / int to char\n",
        "    char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "    int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "    \n",
        "    # Prepare training data, for every <seq_length> chars, predict 1 char after the sequence\n",
        "    n_chars = len(raw_text)\n",
        "    dataX = [] # N x seq_length\n",
        "    dataY = [] # N x 1\n",
        "    for i in range(0, n_chars - seq_length):\n",
        "        seq_in = raw_text[i:i + seq_length]\n",
        "        seq_out = raw_text[i + seq_length]\n",
        "        dataX.append([char_to_int[char] for char in seq_in])\n",
        "        dataY.append(char_to_int[seq_out])\n",
        "    \n",
        "    return (dataX, dataY, char_to_int, int_to_char, chars)\n",
        "\n",
        "def format_data(dataX, dataY, n_classes, batch_size=64):\n",
        "    '''Parse into minibatches, return Tensors'''\n",
        "\n",
        "    # For simplicity, discard trailing data not fitting into batch_size\n",
        "    n_patterns = len(dataY)\n",
        "    n_patterns = n_patterns - n_patterns % batch_size\n",
        "    X = dataX[:n_patterns]\n",
        "    Y = dataY[:n_patterns]\n",
        "\n",
        "    # Parse X\n",
        "    X = np.array(X)\n",
        "    _, seq_length = X.shape\n",
        "    X = X.reshape(-1, batch_size, seq_length)\n",
        "\n",
        "    X = torch.LongTensor(X)\n",
        "\n",
        "    # Parse Y\n",
        "    Y = np.array(Y)\n",
        "    Y = Y.reshape(-1, batch_size)\n",
        "\n",
        "    Y = torch.LongTensor(Y)\n",
        "\n",
        "    return list(zip(X, Y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUHV3pviig4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, n_vocab, embedding_dim, hidden_dim, dropout=0.2):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.embeddings = nn.Embedding(n_vocab, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, dropout=dropout)\n",
        "        self.hidden2out = nn.Linear(hidden_dim, n_vocab)\n",
        "\n",
        "    def forward(self, seq_in):\n",
        "        embeddings = self.embeddings(seq_in.t()) # LSTM takes 3D inputs (timesteps, batch, features)\n",
        "                                                 #                    = (seq_length, batch_size, embedding_dim)\n",
        "        lstm_out, _ = self.lstm(embeddings)      # Each timestep outputs 1 hidden_state\n",
        "                                                 # Combined in lstm_out = (seq_length, batch_size, hidden_dim) \n",
        "        ht = lstm_out[-1]                        # ht = last hidden state = (batch_size, hidden_dim)\n",
        "                                                 # Use the last hidden state to predict the following character\n",
        "        out = self.hidden2out(ht)                # Fully-connected layer, predict (batch_size, n_vocab)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW2ycdejF0C9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def load_pickle(path):\n",
        "    with open(path,'rb') as f:\n",
        "      data = pickle.load(f)\n",
        "      f.close()\n",
        "    return data\n",
        "\n",
        "def is_end(c):\n",
        "    end_tokens = ['。', '？', '！', '.', '?', '!']\n",
        "    return c in end_tokens\n",
        "\n",
        "def to_prob(vec):\n",
        "    s = sum(vec)\n",
        "    return [v / s for v in vec]\n",
        "\n",
        "def gen_text(model, patterns, char_to_int, int_to_char, chars, n_sent=10,restart_seq=False):\n",
        "    n_patterns = len(patterns)\n",
        "\n",
        "    # Randomly choose a pattern to start text generation\n",
        "    start=np.random.randint(0, n_patterns - 1)\n",
        "    #start = np.random.randint(0, n_patterns - 1)\n",
        "    pattern = patterns[start]\n",
        "    # Start generation until n_sent sentences generated \n",
        "    cnt = 0\n",
        "    while cnt < n_sent: \n",
        "        # Format input pattern\n",
        "        seq_in = np.array(pattern)\n",
        "        seq_in = seq_in.reshape(1, -1) # batch_size = 1\n",
        "\n",
        "        seq_in = Variable(torch.LongTensor(seq_in))\n",
        "\n",
        "        # Predict next character\n",
        "        pred = model(seq_in)\n",
        "        pred = to_prob(F.softmax(pred, dim=1).data[0].numpy()) # turn into probability distribution\n",
        "        char = np.random.choice(chars, p=pred)                 # pick char based on probability instead of always picking the highest value\n",
        "        char_idx = char_to_int[char]\n",
        "        print(char, end='')\n",
        "\n",
        "        # Append predicted character to pattern, truncate to usual pattern size, use as new pattern\n",
        "        pattern.append(char_idx)\n",
        "        pattern = pattern[1:]\n",
        "\n",
        "        if is_end(char):\n",
        "            if restart_seq:\n",
        "                start = np.random.randint(0, n_patterns - 1)\n",
        "                pattern = patterns[start]\n",
        "                print()\n",
        "\n",
        "            cnt += 1 \n",
        "    \n",
        "    if not restart_seq:\n",
        "        print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyHzOrEkysCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def load_data(path, seq_length, batch_size):\n",
        "    dataX, dataY, char_to_int, int_to_char, chars = parse_corpus(path, seq_length=seq_length)\n",
        "    data = format_data(dataX, dataY, n_classes=len(chars), batch_size=batch_size)\n",
        "\n",
        "    return data, dataX, dataY, char_to_int, int_to_char, chars\n",
        "\n",
        "def save_pickle(data, path):\n",
        "    with open(path, 'wb') as f:  \n",
        "        pickle.dump(data, f)\n",
        "        f.close()\n",
        "\n",
        "def train(model, optimizer, epoch, data, log_interval):\n",
        "    model.train()\n",
        "\n",
        "    for batch_i, (seq_in, target) in enumerate(data):\n",
        "        seq_in, target = Variable(seq_in), Variable(target)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(seq_in)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Log training status\n",
        "        if batch_i % log_interval == 0:\n",
        "            print('Train epoch: {} ({:2.0f}%)\\tLoss: {:.6f}'.format(epoch, 100. * batch_i / len(data), loss.data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_GaLasNingK",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Prepare\n",
        "    train_data, dataX, dataY, char_to_int, int_to_char, chars = load_data(\"corpus.txt\", seq_length=8, batch_size=16)\n",
        "    model = Net(len(chars), 2, 254, dropout=0.3)\n",
        "    #Net(len(chars),args.embedding_dim, args.hidden_dim, dropout=args.dropout)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "    #torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(100):\n",
        "    #for epoch in range(args.epochs)\n",
        "        train(model, optimizer, epoch, train_data, log_interval=10)\n",
        "        #train(model, optimizer, epoch, train_data, log_interval=args.log_interval)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          #(epoch + 1) % args.save_interval == 0\n",
        "            model.eval()\n",
        "            torch.save(model.state_dict(), \"training_backup.pkl\")#僅儲存參數，而非整個模型\n",
        "            #torch.save(model, args.output)\n",
        "\n",
        " # Save mappings, vocabs, & model\n",
        "    save_pickle((dataX, char_to_int, int_to_char, chars), \"training_save.pkl\")\n",
        "    #save_pickle((dataX, char_to_int, int_to_char, chars), args.output_c)\n",
        "\n",
        "    model.eval()\n",
        "    torch.save(model, \"training_model.pkl\")#儲存整個模型\n",
        "    \n",
        "    # Load model\n",
        "    model=torch.load(\"training_model.pkl\")\n",
        "\n",
        "    # Load mappings & vocabularies\n",
        "    dataX, char_to_int, int_to_char, chars = load_pickle(\"training_save.pkl\")\n",
        "    \n",
        "    # Generate text\n",
        "    gen_text(model, dataX, char_to_int, int_to_char, chars, n_sent=10, restart_seq=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIoe0QK9Sz6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#讀取備分\n",
        "\n",
        "    # Prepare\n",
        "    train_data, dataX, dataY, char_to_int, int_to_char, chars = load_data(\"corpus.txt\", seq_length=8, batch_size=32)\n",
        "    model.load_state_dict(torch.load(\"training_backup.pkl\"))\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "    \n",
        "    # Train\n",
        "    for epoch in range(10):\n",
        "\n",
        "        train(model, optimizer, epoch, train_data, log_interval=10)\n",
        "\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "\n",
        "            model.eval()\n",
        "            torch.save(model, \"training_backup.pkl\")\n",
        "\n",
        "\n",
        " # Save mappings, vocabs, & model\n",
        "    save_pickle((dataX, char_to_int, int_to_char, chars), \"training_save.pkl\")\n",
        "    #save_pickle((dataX, char_to_int, int_to_char, chars), args.output_c)\n",
        "\n",
        "    model.eval()\n",
        "    torch.save(model, \"training_model.pkl\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0dnkNvUudgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#產生文本\n",
        "if __name__ == '__main__':\n",
        " # Load model\n",
        "    model=torch.load(\"training_model.pkl\")\n",
        "\n",
        "    # Load mappings & vocabularies\n",
        "    dataX, char_to_int, int_to_char, chars = load_pickle(\"training_save.pkl\")\n",
        "    \n",
        "    # Generate text\n",
        "    gen_text(model, dataX, char_to_int, int_to_char, chars, n_sent=10, restart_seq=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81O0pYRtN7cC",
        "colab_type": "code",
        "outputId": "a4ae7db0-efa4-4e1e-c19b-05166c4383dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#取得模型資料\n",
        "print(load_pickle(\"training_save.pkl\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}